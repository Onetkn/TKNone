import copy
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

font = {'size':14}
matplotlib.rc('font',**font)

#set up the training data 假设创建训练数据如下

  #2 features
X_train=np.array([[0.25,1.5],[0.5,0.5],[2,0.5],[1.25,1],[3,1],[2,2],[1,2.5],[3,2.5]])
y_train=np.array([0,0,0,0,1,1,1,1])
            ~       ~ku
print(X_train.shape)

m,n=X_train.shape
plt_split=y_train>0.5

for i in range(m):
    if plt_split[i]:
        plt.scatter(X_train[i][0],X_train[i][1],marker='x',c='r',label='positive')
    else:
        plt.scatter(X_train[i][0],X_train[i][1],marker='o',c='g',label='negtive')


def model(x,w,b):
    z=np.dot(x,w)+b
    return z

def sigmoid(z):
    g=1/(1+np.exp(-z))
    return g

#design the cost function

def logistic__cost_function(X_train,y_train,model,activation,w,b):
    m,n=X_train.shape
    cost=0.0
    for i in range(m):
        z_i=model(X_train[i],w,b)
        y_pred=activation(z_i)
        cost +=(-y_train[i])*np.log(y_pred)-(1-y_train[i])*np.log(1-y_pred)
    cost=cost/m
    return cost

w=np.array([1,1])
b=-3
cost=logistic__cost_function(X_train,y_train,model,sigmoid,w,b)

#optimise the model

def gradient_function(X_train, y_train, model, activation, w, b, lam=1):
	m, n = X_train.shape
	dJ_dw = np.zeros((n,))         # double check dimensions
	dJ_db = 0

	y_pred = np.zeros((m,))

	# loop through samples i, from 0 to m-1
	for i in range(m):
		y_pred[i] = activation(model(X_train[i],w,b))

		# loop through features j, from 0 to n-1
		for j in range(n):
			dJ_dw[j] = dJ_dw[j] + (y_pred[i] - y_train[i])*X_train[i, j]
		dJ_db = dJ_db + (y_pred[i] - y_train[i])
	dJ_dw = dJ_dw / m
	dJ_db = dJ_db / m

	for j in range(n):
		dJ_dw[j] += lam/m*w[j]

	return dJ_dw, dJ_db

dJ_dw,dJ_db=gradient_function(X_train,y_train,model,sigmoid,w,b)

def gradient_descent(X_train, y_train, w_init, b_init, alpha, N_iterations, model, activation, cost_function, gradient_function):

    J_log = []
    w = copy.deepcopy(w_init)
    b = b_init

    for i in range(N_iterations):

        dJ_dw, dJ_db = gradient_function(X_train, y_train, model, activation, w, b)

        w = w - alpha * dJ_dw
        b = b - alpha * dJ_db

        if i < 100000:
            J_log.append(cost_function(X_train, y_train, model, activation, w, b))

    return w, b, J_log

n = X_train.shape[1]
print(n)
w_init = np.zeros((n,))
b_init = 0.0
N_iterations = 10000
alpha = 0.01

w_final, b_final, J_log = gradient_descent(X_train, y_train, w_init, b_init, alpha, N_iterations, model, sigmoid,logistic__cost_function, gradient_function)

f04 = plt.figure(4)
plt.plot(J_log)
plt.xlabel('number of iterations', fontsize=14)
plt.ylabel('cost function', fontsize=14)
plt.show()
